{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqpsYn49QWSR"
      },
      "source": [
        "#Introducing Naive, Advanced, and Modular RAG\n",
        "\n",
        "Copyright 2024, Denis Rothman\n",
        "\n",
        "This notebook introduces Naïve, Advanced, and Modular RAG through basic educational examples.\n",
        "\n",
        "The Naïve, Advanced and modular RAG techniques offer flexibility in selecting retrieval strategies, allowing adaptation to various tasks and data characteristics.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "**Part 1: Foundations and Basic Implementation**\n",
        "\n",
        "1.Environment setup for OpenAI API integration  \n",
        "2.Generator function using GPT models    \n",
        "3.Dataetup with a list of documents (db_records)  \n",
        "4.Query(user request)  \n",
        "\n",
        "**Part 2: Advanced Techniques and Evaluation**\n",
        "\n",
        "1.Retrieval metrics  \n",
        "2.Naive RAG  \n",
        "3.Advanced RAG  \n",
        "4.Modular RAG Retriever  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ICSQQ0ipxlR"
      },
      "source": [
        "# Part 1: Foundations and Basic Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o01-IM8bTc5f"
      },
      "source": [
        "# 1.The Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VCfbN0YwHbE",
        "outputId": "47dbbe95-1ee2-4bde-fd96-cf34b7f8e157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.40.3\n",
            "  Downloading openai-1.40.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai==1.40.3) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai==1.40.3) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.40.3) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.40.3) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.40.3) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.40.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.40.3) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.40.3) (0.4.1)\n",
            "Downloading openai-1.40.3-py3-none-any.whl (360 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.7/360.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.97.0\n",
            "    Uninstalling openai-1.97.0:\n",
            "      Successfully uninstalled openai-1.97.0\n",
            "Successfully installed openai-1.40.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.40.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myXJn33zbqTR",
        "outputId": "f64a3365-609e-4097-bf47-518fdef1b27e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#API Key\n",
        "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Oefvqp21Ba07"
      },
      "outputs": [],
      "source": [
        "f = open(\"drive/MyDrive/OPENAI_API_KEY.txt\", \"r\")\n",
        "API_KEY=f.readline().strip()\n",
        "f.close()\n",
        "\n",
        "#The OpenAI Key\n",
        "import os\n",
        "import openai\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuZ7jr4Rs36U"
      },
      "source": [
        "# 2.The Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "ZY9qHPvhv110",
        "outputId": "5501548d-3d5d-454f-a911-b3fda6b330f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.40.3)\n",
            "Collecting openai\n",
            "  Downloading openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Downloading openai-1.97.1-py3-none-any.whl (764 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.4/764.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.40.3\n",
            "    Uninstalling openai-1.40.3:\n",
            "      Successfully uninstalled openai-1.40.3\n",
            "Successfully installed openai-1.97.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "d50701f889464b56b1886179c0bebe2e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qwCNTW9fs36U"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "gptmodel=\"gpt-4o\"\n",
        "\n",
        "def call_llm_with_full_text(itext):\n",
        "    # Join all lines to form a single string\n",
        "    text_input = '\\n'.join(itext)\n",
        "    prompt = f\"Please elaborate on the following content:\\n{text_input}\"\n",
        "\n",
        "    try:\n",
        "      response = client.chat.completions.create(\n",
        "         model=gptmodel,\n",
        "         messages=[\n",
        "            # {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert. Always explain clearly, with examples, and reply in Russian.\"},\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert Natural Language Processing exercise expert\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "         ],\n",
        "         temperature=0.1  # Add the temperature parameter here and other parameters you need\n",
        "        )\n",
        "      return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return str(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVKe9VF0HIHJ"
      },
      "source": [
        "## Formatted response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oG8I2Kb2HFhL"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def print_formatted_response(response):\n",
        "    # Define the width for wrapping the text\n",
        "    wrapper = textwrap.TextWrapper(width=80)  # Set to 80 columns wide, but adjust as needed\n",
        "    wrapped_text = wrapper.fill(text=response)\n",
        "\n",
        "    # Print the formatted response with a header and footer\n",
        "    print(\"Response:\")\n",
        "    print(\"---------------\")\n",
        "    print(wrapped_text)\n",
        "    print(\"---------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv1ExPiZdJRL"
      },
      "source": [
        " # 3.The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "45CFxG4Fgcju"
      },
      "outputs": [],
      "source": [
        "db_records = [\n",
        "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
        "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
        "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
        "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
        "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
        "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
        "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
        "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
        "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
        "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
        "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
        "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
        "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
        "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
        "    \"The retrieved documents are then fed into the language model.\",\n",
        "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
        "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
        "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
        "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
        "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
        "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
        "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
        "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
        "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
        "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
        "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
        "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
        "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIQ7NK92g7EC",
        "outputId": "3126136e-1bca-47ce-ec26-e477d86827cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
            "in the field of artificial intelligence, particularly within the realm of\n",
            "natural language processing (NLP). It innovatively combines the capabilities of\n",
            "neural network-based language models with retrieval systems to enhance the\n",
            "generation of text, making it more accurate, informative, and contextually\n",
            "relevant. This methodology leverages the strengths of both generative and\n",
            "retrieval architectures to tackle complex tasks that require not only linguistic\n",
            "fluency but also factual correctness and depth of knowledge. At the core of\n",
            "Retrieval Augmented Generation (RAG) is a generative model, typically a\n",
            "transformer-based neural network, similar to those used in models like GPT\n",
            "(Generative Pre-trained Transformer) or BERT (Bidirectional Encoder\n",
            "Representations from Transformers). This component is responsible for producing\n",
            "coherent and contextually appropriate language outputs based on a mixture of\n",
            "input prompts and additional information fetched by the retrieval component.\n",
            "Complementing the language model is the retrieval system, which is usually built\n",
            "on a database of documents or a corpus of texts. This system uses techniques\n",
            "from information retrieval to find and fetch documents that are relevant to the\n",
            "input query or prompt. The mechanism of relevance determination can range from\n",
            "simple keyword matching to more complex semantic search algorithms which\n",
            "interpret the meaning behind the query to find the best matches. This component\n",
            "merges the outputs from the language model and the retrieval system. It\n",
            "effectively synthesizes the raw data fetched by the retrieval system into the\n",
            "generative process of the language model. The integrator ensures that the\n",
            "information from the retrieval system is seamlessly incorporated into the final\n",
            "text output, enhancing the model's ability to generate responses that are not\n",
            "only fluent and grammatically correct but also rich in factual details and\n",
            "context-specific nuances. When a query or prompt is received, the system first\n",
            "processes it to understand the requirement or the context. Based on the\n",
            "processed query, the retrieval system searches through its database to find\n",
            "relevant documents or information snippets. This retrieval is guided by the\n",
            "similarity of content in the documents to the query, which can be determined\n",
            "through various techniques like vector embeddings or semantic similarity\n",
            "measures. The retrieved documents are then fed into the language model. In some\n",
            "implementations, this integration happens at the token level, where the model\n",
            "can access and incorporate specific pieces of information from the retrieved\n",
            "texts dynamically as it generates each part of the response. The language model,\n",
            "now augmented with direct access to retrieved information, generates a response.\n",
            "This response is not only influenced by the training of the model but also by\n",
            "the specific facts and details contained in the retrieved documents, making it\n",
            "more tailored and accurate. By directly incorporating information from external\n",
            "sources, Retrieval Augmented Generation (RAG) models can produce responses that\n",
            "are more factual and relevant to the given query. This is particularly useful in\n",
            "domains like medical advice, technical support, and other areas where precision\n",
            "and up-to-date knowledge are crucial. Retrieval Augmented Generation (RAG)\n",
            "systems can dynamically adapt to new information since they retrieve data in\n",
            "real-time from their databases. This allows them to remain current with the\n",
            "latest knowledge and trends without needing frequent retraining. With access to\n",
            "a wide range of documents, Retrieval Augmented Generation (RAG) systems can\n",
            "provide detailed and nuanced answers that a standalone language model might not\n",
            "be capable of generating based solely on its pre-trained knowledge. While\n",
            "Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes\n",
            "with its challenges. These include the complexity of integrating retrieval and\n",
            "generation systems, the computational overhead associated with real-time data\n",
            "retrieval, and the need for maintaining a large, up-to-date, and high-quality\n",
            "database of retrievable texts. Furthermore, ensuring the relevance and accuracy\n",
            "of the retrieved information remains a significant challenge, as does managing\n",
            "the potential for introducing biases or errors from the external sources. In\n",
            "summary, Retrieval Augmented Generation represents a significant advancement in\n",
            "the field of artificial intelligence, merging the best of retrieval-based and\n",
            "generative technologies to create systems that not only understand and generate\n",
            "natural language but also deeply comprehend and utilize the vast amounts of\n",
            "information available in textual form. A RAG vector store is a database or\n",
            "dataset that contains vectorized data points.\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "paragraph = ' '.join(db_records)\n",
        "wrapped_text = textwrap.fill(paragraph, width=80)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL7cHuuLhQ5w"
      },
      "source": [
        "# 4.The Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qARk6gtohSXW"
      },
      "outputs": [],
      "source": [
        "query = \"define a rag store\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iITo3QIF7yeK"
      },
      "source": [
        "Generation without augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXBILWI47yeM",
        "outputId": "345ef251-564a-4760-c19d-d2ddbbe58f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Конечно! Давайте разберем, что такое \"аргстор\" (argstore) в контексте\n",
            "программирования и обработки данных.  **Аргстор (argstore)** — это термин,\n",
            "который может использоваться для обозначения структуры или механизма,\n",
            "предназначенного для хранения аргументов, передаваемых функции или программе.\n",
            "Это может быть полезно в различных сценариях, например, при обработке командной\n",
            "строки, где необходимо сохранить и управлять множеством параметров.  ### Пример\n",
            "использования:  Представьте, что у вас есть программа, которая принимает\n",
            "несколько аргументов командной строки. Вы можете использовать структуру\n",
            "\"аргстор\" для их хранения и последующей обработки.  ```python import argparse\n",
            "def main():     parser = argparse.ArgumentParser(description='Пример программы с\n",
            "аргументами.')     parser.add_argument('--input', type=str, help='Путь к\n",
            "входному файлу')     parser.add_argument('--output', type=str, help='Путь к\n",
            "выходному файлу')     parser.add_argument('--verbose', action='store_true',\n",
            "help='Включить подробный вывод')      args = parser.parse_args()      if\n",
            "args.verbose:         print(f\"Входной файл: {args.input}\")\n",
            "print(f\"Выходной файл: {args.output}\")  if __name__ == \"__main__\":     main()\n",
            "```  ### Объяснение:  1. **argparse**: Это модуль в Python, который позволяет\n",
            "легко обрабатывать аргументы командной строки. Он автоматически создает\n",
            "интерфейс командной строки и помогает управлять аргументами.  2.\n",
            "**parser.add_argument()**: Этот метод используется для добавления аргументов,\n",
            "которые программа может принимать. В примере выше мы добавили три аргумента:\n",
            "`--input`, `--output` и `--verbose`.  3. **args**: Это объект, который хранит\n",
            "значения всех аргументов, переданных программе. Вы можете получить доступ к\n",
            "каждому аргументу через его имя, как показано в примере.  Таким образом,\n",
            "\"аргстор\" может быть полезен для упрощения работы с аргументами и параметрами,\n",
            "обеспечивая удобный способ их хранения и использования в программе.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(query)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response"
      ],
      "metadata": {
        "id": "IBj-JKiT03UE",
        "outputId": "4c1d42d0-99e9-4e3e-f418-d1726b018ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Конечно! Давайте разберем, что такое \"аргстор\" (argstore) в контексте программирования и обработки данных.\\n\\n**Аргстор (argstore)** — это термин, который может использоваться для обозначения структуры или механизма, предназначенного для хранения аргументов, передаваемых функции или программе. Это может быть полезно в различных сценариях, например, при обработке командной строки, где необходимо сохранить и управлять множеством параметров.\\n\\n### Пример использования:\\n\\nПредставьте, что у вас есть программа, которая принимает несколько аргументов командной строки. Вы можете использовать структуру \"аргстор\" для их хранения и последующей обработки.\\n\\n```python\\nimport argparse\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\\'Пример программы с аргументами.\\')\\n    parser.add_argument(\\'--input\\', type=str, help=\\'Путь к входному файлу\\')\\n    parser.add_argument(\\'--output\\', type=str, help=\\'Путь к выходному файлу\\')\\n    parser.add_argument(\\'--verbose\\', action=\\'store_true\\', help=\\'Включить подробный вывод\\')\\n\\n    args = parser.parse_args()\\n\\n    if args.verbose:\\n        print(f\"Входной файл: {args.input}\")\\n        print(f\"Выходной файл: {args.output}\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Объяснение:\\n\\n1. **argparse**: Это модуль в Python, который позволяет легко обрабатывать аргументы командной строки. Он автоматически создает интерфейс командной строки и помогает управлять аргументами.\\n\\n2. **parser.add_argument()**: Этот метод используется для добавления аргументов, которые программа может принимать. В примере выше мы добавили три аргумента: `--input`, `--output` и `--verbose`.\\n\\n3. **args**: Это объект, который хранит значения всех аргументов, переданных программе. Вы можете получить доступ к каждому аргументу через его имя, как показано в примере.\\n\\nТаким образом, \"аргстор\" может быть полезен для упрощения работы с аргументами и параметрами, обеспечивая удобный способ их хранения и использования в программе.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11HLkKQMqaDt"
      },
      "source": [
        "# Part 2: Advanced Techniques and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMGuZg1WiaUE"
      },
      "source": [
        "# 1.Retrieval Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHN6s7wZirQL"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Как использовать русские стоп-слова?\n",
        "# pip install nltk\n"
      ],
      "metadata": {
        "id": "Z96a1_Rv_mtd",
        "outputId": "fb90af45-231e-441b-f57c-8fad22426a8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "vectorizer_russian = TfidfVectorizer(\n",
        "    stop_words=russian_stopwords,\n",
        "    ngram_range=(1, 2),\n",
        "    sublinear_tf=True,\n",
        "    norm='l2',\n",
        ")"
      ],
      "metadata": {
        "id": "ipJRXrY8_ubv",
        "outputId": "3b5d3900-c723-4874-f7a2-0e71f5d13dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_GLECrTQirQN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_similarity(text1, text2):\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words='english',\n",
        "        use_idf=True,\n",
        "        norm='l2',\n",
        "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
        "        sublinear_tf=True,   # Apply sublinear TF scaling\n",
        "        analyzer='word'      # You could also experiment with 'char' or 'char_wb' for character-level features\n",
        "    )\n",
        "    tfidf = vectorizer.fit_transform([text1, text2])\n",
        "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
        "    return similarity[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJOi-jrjI5A"
      },
      "source": [
        "## Enhanced Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnmPG9UWjJXD",
        "outputId": "f42bdd69-8aa6-4935-a0b2-71ee26a18c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return synonyms\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    lemmatized_words = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        lemmatized_words.append(token.lemma_)\n",
        "    return lemmatized_words\n",
        "\n",
        "def expand_with_synonyms(words):\n",
        "    expanded_words = words.copy()\n",
        "    for word in words:\n",
        "        expanded_words.extend(get_synonyms(word))\n",
        "    return expanded_words\n",
        "\n",
        "def calculate_enhanced_similarity(text1, text2):\n",
        "    # Preprocess and tokenize texts\n",
        "    words1 = preprocess_text(text1)\n",
        "    words2 = preprocess_text(text2)\n",
        "\n",
        "    # Expand with synonyms\n",
        "    words1_expanded = expand_with_synonyms(words1)\n",
        "    words2_expanded = expand_with_synonyms(words2)\n",
        "\n",
        "    # Count word frequencies\n",
        "    freq1 = Counter(words1_expanded)\n",
        "    freq2 = Counter(words2_expanded)\n",
        "\n",
        "    # Create a set of all unique words\n",
        "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
        "\n",
        "    # Create frequency vectors\n",
        "    vector1 = [freq1[word] for word in unique_words]\n",
        "    vector2 = [freq2[word] for word in unique_words]\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFqh9rr81SUn"
      },
      "source": [
        "# 2.Naive RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu8vteKmS_qO"
      },
      "source": [
        "## Keyword search and matching"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query"
      ],
      "metadata": {
        "id": "saLibmtKOZZf",
        "outputId": "7a68b323-db3e-4a66-956c-13d48bd0556e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'define a rag store'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db_records"
      ],
      "metadata": {
        "id": "_OHRVlrQOnE9",
        "outputId": "8b133492-475d-4bb4-8d2f-81432a19b2d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).',\n",
              " 'It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.',\n",
              " 'This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.',\n",
              " 'At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).',\n",
              " 'This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.',\n",
              " 'Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.',\n",
              " 'This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.',\n",
              " 'The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.',\n",
              " 'This component merges the outputs from the language model and the retrieval system.',\n",
              " 'It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.',\n",
              " \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
              " 'When a query or prompt is received, the system first processes it to understand the requirement or the context.',\n",
              " 'Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.',\n",
              " 'This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.',\n",
              " 'The retrieved documents are then fed into the language model.',\n",
              " 'In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.',\n",
              " 'The language model, now augmented with direct access to retrieved information, generates a response.',\n",
              " 'This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.',\n",
              " 'By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.',\n",
              " 'This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.',\n",
              " 'Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.',\n",
              " 'This allows them to remain current with the latest knowledge and trends without needing frequent retraining.',\n",
              " 'With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.',\n",
              " 'While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.',\n",
              " 'These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.',\n",
              " 'Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.',\n",
              " 'In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.',\n",
              " 'A RAG vector store is a database or dataset that contains vectorized data points.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY1JU0Ush_l4",
        "outputId": "1346265e-d96a-4adb-81bd-b71d1cb82f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Keyword Score: 3\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def find_best_match_keyword_search(query, db_records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "\n",
        "    # Split the query into individual keywords\n",
        "    query_keywords = set(query.lower().split())\n",
        "\n",
        "    # Iterate through each record in db_records\n",
        "    for record in db_records:\n",
        "        # Split the record into keywords\n",
        "        record_keywords = set(record.lower().split())\n",
        "\n",
        "        # Calculate the number of common keywords\n",
        "        common_keywords = query_keywords.intersection(record_keywords)\n",
        "        current_score = len(common_keywords)\n",
        "\n",
        "        # Update the best score and record if the current score is higher\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "\n",
        "    return best_score, best_record\n",
        "\n",
        "# Assuming 'query' and 'db_records' are defined in previous cells in your Colab notebook\n",
        "best_keyword_score, best_matching_record = find_best_match_keyword_search(query, db_records)\n",
        "\n",
        "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oak-3k_dkzC3"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blcPOIaHkzC4",
        "outputId": "394f56ea-e803-40ff-ac57-35fa660a6ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Cosine Similarity Score: 0.126\n"
          ]
        }
      ],
      "source": [
        "# Cosine Similarity\n",
        "score = calculate_cosine_similarity(query, best_matching_record)\n",
        "print(f\"Best Cosine Similarity Score: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OW0l24IkzC5",
        "outputId": "642af1be-bb06-4262-9d79-67d131c35e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity:, 0.642\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Similarity\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2zKQhiO0Fcr"
      },
      "source": [
        "## Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "r_7ymSxG0Fcs"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+ \": \"+ best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NTfxzum0PT2",
        "outputId": "4be70f22-3d14-42bc-87db-14fc455c1a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ui8wH4k3_g4"
      },
      "source": [
        "## Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh8BsnUy0Fcs",
        "outputId": "bc6ce6f1-7539-47c9-b48e-b1b778dd8f5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "---------------\n",
            "Certainly! Let's break down the concept of an \"ARAG vector store\" as described\n",
            "in your input.  ### ARAG Vector Store  1. **Definition**:    - An ARAG vector\n",
            "store is essentially a specialized type of database or dataset. Its primary\n",
            "function is to store data in a vectorized format.  2. **Vectorized Data\n",
            "Points**:    - **Vectors**: In the context of data storage and processing, a\n",
            "vector is a mathematical representation of data points. Vectors are often used\n",
            "in machine learning and data science to represent features of data in a\n",
            "numerical format that algorithms can process.    - **Vectorization**: This is\n",
            "the process of converting data into a vector format. For example, in natural\n",
            "language processing (NLP), text data is often vectorized using techniques like\n",
            "word embeddings (e.g., Word2Vec, GloVe) or more advanced methods like BERT\n",
            "embeddings.  3. **Purpose and Use Cases**:    - **Efficient Data Retrieval**:\n",
            "Vector stores are designed to facilitate efficient retrieval and manipulation of\n",
            "data. They are particularly useful in scenarios where similarity searches are\n",
            "required, such as finding similar documents, images, or other data points.    -\n",
            "**Machine Learning**: Vector stores are commonly used in machine learning\n",
            "applications where data needs to be processed in a format that algorithms can\n",
            "easily work with. This includes tasks like clustering, classification, and\n",
            "recommendation systems.    - **Scalability**: These stores are optimized to\n",
            "handle large volumes of vectorized data, making them suitable for big data\n",
            "applications.  4. **Examples and Technologies**:    - There are several\n",
            "technologies and frameworks that provide vector storage capabilities. Examples\n",
            "include FAISS (Facebook AI Similarity Search), Annoy (Approximate Nearest\n",
            "Neighbors Oh Yeah), and Elasticsearch with vector capabilities.    - These\n",
            "technologies often provide functionalities like approximate nearest neighbor\n",
            "(ANN) search, which allows for fast and efficient querying of similar vectors.\n",
            "5. **Applications**:    - **Search Engines**: Enhancing search capabilities by\n",
            "understanding the semantic meaning of queries and documents.    -\n",
            "**Recommendation Systems**: Providing personalized recommendations based on user\n",
            "preferences and behavior.    - **Image and Video Analysis**: Identifying and\n",
            "categorizing visual content based on feature vectors.  In summary, an ARAG\n",
            "vector store is a powerful tool for managing and utilizing vectorized data,\n",
            "enabling advanced data processing and retrieval capabilities in various\n",
            "applications.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJH2__0iTUr1"
      },
      "source": [
        "# 3.Advanced RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awyjcn35jFiy"
      },
      "source": [
        "## 3.1.Vector search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD8_758kkq3h"
      },
      "source": [
        "### Search function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCBbY4qLc8qh"
      },
      "outputs": [],
      "source": [
        "def find_best_match(text_input, records):\n",
        "    best_score = 0\n",
        "    best_record = None\n",
        "    for record in records:\n",
        "        current_score = calculate_cosine_similarity(text_input, record)\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_record = record\n",
        "    return best_score, best_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG1iM-U33OCg"
      },
      "outputs": [],
      "source": [
        "best_similarity_score, best_matching_record = find_best_match(query, db_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLa9NQ4Cm_YQ",
        "outputId": "7aac4922-3c63-48a0-afa4-32580bedbc5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A60QoOA3jf9j"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIUh-38knHLI",
        "outputId": "11f8eeb6-7cd3-40bc-c6ad-fe03332f5f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Cosine Similarity Score: 0.126\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQopW_FSjBSr",
        "outputId": "75cef793-36ee-4dee-ca1a-6d41844654b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity:, 0.642\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Similarity\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, best_matching_record)\n",
        "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51fZC6Oe2G9E"
      },
      "source": [
        "### Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dcnK7OGx5e6"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+\": \"+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3uk-91x049J",
        "outputId": "81eed858-e9f5-45ef-e603-1c9374490398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFDF6hbi2LF9"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJC-mA5ftxFU",
        "outputId": "1358b6e7-da69-45b5-b22d-818f489be893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "---------------\n",
            "An \"ARAG vector store\" refers to a type of database or dataset specifically\n",
            "designed to store and manage vectorized data points. Let's break down the key\n",
            "components of this concept:  1. **Vector Store**: A vector store is a\n",
            "specialized data storage system that focuses on handling data in vector form.\n",
            "Vectors are mathematical representations of data points, often used in machine\n",
            "learning and data science to represent features or characteristics of data in a\n",
            "numerical format. These vectors can be used for various purposes, such as\n",
            "similarity searches, clustering, and classification.  2. **ARAG**: While the\n",
            "term \"ARAG\" is not widely recognized in the context of vector stores, it could\n",
            "potentially refer to a specific implementation, framework, or methodology\n",
            "related to vector storage. Without additional context, it's challenging to\n",
            "provide a precise definition of \"ARAG\" in this context. It might be an acronym\n",
            "or a proprietary name for a particular system or approach.  3. **Database or\n",
            "Dataset**: The vector store functions as a database or dataset, meaning it is a\n",
            "structured collection of data that can be queried, retrieved, and manipulated.\n",
            "In the case of a vector store, the data is organized in a way that optimizes the\n",
            "storage and retrieval of vectorized information.  4. **Vectorized Data Points**:\n",
            "These are data points that have been transformed into vectors. Vectorization is\n",
            "the process of converting data into a numerical format that can be easily\n",
            "processed by algorithms. This is common in fields like natural language\n",
            "processing, where text data is converted into numerical vectors using techniques\n",
            "like word embeddings or TF-IDF.  In summary, an ARAG vector store is a system\n",
            "designed to efficiently store and manage data that has been converted into\n",
            "vectors, facilitating operations like similarity searches and machine learning\n",
            "tasks. The specific characteristics of an \"ARAG\" vector store would depend on\n",
            "the implementation details and the context in which it is used.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7djpPBpm0M2"
      },
      "source": [
        "## 3.2.Index-based search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyDUhy_1lBfT"
      },
      "source": [
        "### Search Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRarT_fym2XC",
        "outputId": "9504537f-2354-48bd-de30-f5b4d48b789f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def find_best_match(query, vectorizer, tfidf_matrix):\n",
        "    query_tfidf = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_tfidf, tfidf_matrix)\n",
        "    best_index = similarities.argmax()  # Get the index of the highest similarity score\n",
        "    best_score = similarities[0, best_index]\n",
        "    return best_score, best_index\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)\n",
        "\n",
        "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
        "best_matching_record = db_records[best_index]\n",
        "\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt3iBtJFj4sa"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoNUQqx5j3r4",
        "outputId": "96893316-9ce5-4a2b-9706-20bef4cf1e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Cosine Similarity Score: 0.407\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cosine Similarity\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg910Mhuj3sO",
        "outputId": "262b207e-5ecb-4441-fc15-656cbd0a7ffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity:, 0.642\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Similarity\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(f\"Enhanced Similarity:, {similarity_score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubm0DTxKeqR9"
      },
      "source": [
        "Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbokQ2eacHjM",
        "outputId": "52a1e0f4-ff6c-44ed-8d45-8f1153fc8c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     ability    access  accuracy  accurate     adapt  additional  advancement  \\\n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "1   0.000000  0.000000  0.000000  0.216364  0.000000    0.000000     0.000000   \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000    0.236479     0.000000   \n",
            "5   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "7   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "8   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "9   0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "10  0.186734  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "11  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "12  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "13  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "14  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "15  0.000000  0.172624  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "16  0.000000  0.317970  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "17  0.000000  0.000000  0.000000  0.206861  0.000000    0.000000     0.000000   \n",
            "18  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "19  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "20  0.000000  0.000000  0.000000  0.000000  0.275802    0.000000     0.000000   \n",
            "21  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "22  0.000000  0.174772  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "23  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "24  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "25  0.000000  0.000000  0.228743  0.000000  0.000000    0.000000     0.000000   \n",
            "26  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.173327   \n",
            "27  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000     0.000000   \n",
            "\n",
            "      advice  algorithms    allows  ...    vector  vectorized      when  \\\n",
            "0   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "1   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "2   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "3   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "4   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "5   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "6   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "7   0.000000    0.220687  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "8   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "9   0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "10  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "11  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.295573   \n",
            "12  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "13  0.000000    0.000000  0.000000  ...  0.200131     0.00000  0.000000   \n",
            "14  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "15  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "16  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "17  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "18  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "19  0.244401    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "20  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "21  0.000000    0.000000  0.291503  ...  0.000000     0.00000  0.000000   \n",
            "22  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "23  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "24  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "25  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "26  0.000000    0.000000  0.000000  ...  0.000000     0.00000  0.000000   \n",
            "27  0.000000    0.000000  0.000000  ...  0.307719     0.34589  0.000000   \n",
            "\n",
            "       where     which    while     wide      with    within   without  \n",
            "0   0.000000  0.000000  0.00000  0.00000  0.000000  0.260582  0.000000  \n",
            "1   0.000000  0.000000  0.00000  0.00000  0.160278  0.000000  0.000000  \n",
            "2   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "3   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "4   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "5   0.000000  0.247710  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "6   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "7   0.000000  0.179053  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "8   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "9   0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "10  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "11  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "12  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "13  0.000000  0.182517  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "14  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "15  0.189283  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "16  0.000000  0.000000  0.00000  0.00000  0.258278  0.000000  0.000000  \n",
            "17  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "18  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "19  0.217430  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "20  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "21  0.000000  0.000000  0.00000  0.00000  0.192110  0.000000  0.291503  \n",
            "22  0.000000  0.000000  0.00000  0.21541  0.141963  0.000000  0.000000  \n",
            "23  0.000000  0.000000  0.32932  0.00000  0.217033  0.000000  0.000000  \n",
            "24  0.000000  0.000000  0.00000  0.00000  0.134513  0.000000  0.000000  \n",
            "25  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "26  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "27  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[28 rows x 297 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def setup_vectorizer(records):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(records)\n",
        "\n",
        "    # Convert the TF-IDF matrix to a DataFrame for display purposes\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(tfidf_df)\n",
        "\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "vectorizer, tfidf_matrix = setup_vectorizer(db_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dABZ12Bkugtt"
      },
      "source": [
        "### Augmented input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w4wppuA4eNn"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+\": \"+best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNozI65K4e7u",
        "outputId": "9a909e7f-bd6d-450c-dd2c-a96b8ec4ad62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store: A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU998zkD4hpD"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy9X-l_Iugtt",
        "outputId": "508c8c2c-e2a6-48fe-bfb0-ccd4add39e74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "---------------\n",
            "An ARAG vector store, or more generally a vector store, is a specialized type of\n",
            "database or dataset designed to store and manage vectorized data points. These\n",
            "data points are typically represented as vectors, which are mathematical\n",
            "entities that have both magnitude and direction. In the context of data storage\n",
            "and retrieval, vectors are often used to represent complex data in a numerical\n",
            "format that can be easily processed by algorithms, particularly in machine\n",
            "learning and artificial intelligence applications.  Here's a more detailed\n",
            "explanation of the key components and concepts involved in a vector store:  1.\n",
            "**Vectorization**: This is the process of converting data into a numerical\n",
            "format that can be represented as vectors. For example, text data can be\n",
            "vectorized using techniques like word embeddings (e.g., Word2Vec, GloVe) or\n",
            "sentence embeddings (e.g., BERT, Sentence Transformers), where each word or\n",
            "sentence is represented as a vector in a high-dimensional space.  2. **Data\n",
            "Points**: In a vector store, each data point is represented as a vector. These\n",
            "data points can come from various sources, such as text, images, audio, or any\n",
            "other type of data that can be transformed into a vector format. The vector\n",
            "representation captures the essential features of the data, allowing for\n",
            "efficient storage, retrieval, and analysis.  3. **Storage and Retrieval**: A\n",
            "vector store is optimized for storing large volumes of vectorized data and\n",
            "providing efficient retrieval mechanisms. This is particularly important for\n",
            "applications that require fast similarity searches, such as recommendation\n",
            "systems, information retrieval, and clustering.  4. **Similarity Search**: One\n",
            "of the primary uses of a vector store is to perform similarity searches. This\n",
            "involves finding vectors in the store that are similar to a given query vector.\n",
            "Similarity is often measured using distance metrics like cosine similarity,\n",
            "Euclidean distance, or Manhattan distance. This capability is crucial for tasks\n",
            "like nearest neighbor search, where the goal is to find the most similar data\n",
            "points to a given input.  5. **Applications**: Vector stores are widely used in\n",
            "various applications, including natural language processing, computer vision,\n",
            "and recommendation systems. For instance, in NLP, vector stores can be used to\n",
            "store word or sentence embeddings for tasks like semantic search, document\n",
            "clustering, and topic modeling.  Overall, an ARAG vector store is a powerful\n",
            "tool for managing and utilizing vectorized data, enabling efficient processing\n",
            "and analysis of complex datasets in various domains.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWEvzcDHTX6i"
      },
      "source": [
        "# 4.Modular RAG\n",
        "\n",
        "Modular RAG can combine methods. For example:\n",
        "\n",
        "**keyword search**:Searches through each document to find the one that best matches the keyword(s).\n",
        "\n",
        "**vector search**: Searches through each document and calculates similarity.\n",
        "\n",
        "**indexed search**: Uses a precomputed index (TF-IDF matrix) to compute cosine similarities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv-VqmLf3EQ9"
      },
      "source": [
        "**October 25, 2025 update**\n",
        "\n",
        "`self.documents` is initialized in the fit method to hold the records used for searching and enable the `keyword_search` function to access them without error.\n",
        "\n",
        "**Note on Vector search**\n",
        "\n",
        "In this case, the `def vector_search(self, query):` uses `tfidf_matrix`to increase the vector search performance.\n",
        "\n",
        "The `def vector_search(self, query):` function could use a brute-force method as implemented in `Section 3.1.Vector search` of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18wmqwJd4o62"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class RetrievalComponent:\n",
        "    def __init__(self, method='vector'):\n",
        "        self.method = method\n",
        "        if self.method == 'vector' or self.method == 'indexed':\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def fit(self, records):\n",
        "      self.documents = records  # Initialize self.documents here\n",
        "      if self.method == 'vector' or self.method == 'indexed':\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(records)\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        if self.method == 'keyword':\n",
        "            return self.keyword_search(query)\n",
        "        elif self.method == 'vector':\n",
        "            return self.vector_search(query)\n",
        "        elif self.method == 'indexed':\n",
        "            return self.indexed_search(query)\n",
        "\n",
        "    def keyword_search(self, query):\n",
        "        best_score = 0\n",
        "        best_record = None\n",
        "        query_keywords = set(query.lower().split())\n",
        "        for index, doc in enumerate(self.documents):\n",
        "            doc_keywords = set(doc.lower().split())\n",
        "            common_keywords = query_keywords.intersection(doc_keywords)\n",
        "            score = len(common_keywords)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_record = self.documents[index]\n",
        "        return best_record\n",
        "\n",
        "    def vector_search(self, query):\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]\n",
        "\n",
        "    def indexed_search(self, query):\n",
        "        # Assuming the tfidf_matrix is precomputed and stored\n",
        "        query_tfidf = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)\n",
        "        best_index = similarities.argmax()\n",
        "        return db_records[best_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qHm4saJ8cGk"
      },
      "source": [
        "### Modular RAG Strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kvhIOdY8amp",
        "outputId": "c0a631da-db68-4023-d7b1-dff0586c1dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Usage example\n",
        "retrieval = RetrievalComponent(method='vector')  # Choose from 'keyword', 'vector', 'indexed'\n",
        "retrieval.fit(db_records)\n",
        "best_matching_record = retrieval.retrieve(query)\n",
        "\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgxXdqzvkYDk"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COyYme4IkYDx",
        "outputId": "f6fda5ac-021c-4d98-fe61-b58b5d356dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Cosine Similarity Score: 0.407\n",
            "Response:\n",
            "---------------\n",
            "A RAG vector store is a database or dataset that contains vectorized data\n",
            "points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cosine Similarity\n",
        "print(f\"Best Cosine Similarity Score: {best_similarity_score:.3f}\")\n",
        "print_formatted_response(best_matching_record)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YRTpIpzkYDx",
        "outputId": "f35ce40d-5b4e-4917-a9c1-1305df3cc800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "define a rag store :  A RAG vector store is a database or dataset that contains vectorized data points.\n",
            "Enhanced Similarity: 0.641582812483307\n"
          ]
        }
      ],
      "source": [
        "# Enhanced Similarity\n",
        "response = best_matching_record\n",
        "print(query,\": \", response)\n",
        "similarity_score = calculate_enhanced_similarity(query, response)\n",
        "print(\"Enhanced Similarity:\", similarity_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TaQa7Dc7JwT"
      },
      "source": [
        "### Augmented Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-hKjhIU7Jwg"
      },
      "outputs": [],
      "source": [
        "augmented_input=query+ \" \"+ best_matching_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhSO-fyZ7Jwg",
        "outputId": "3e17c370-ab59-4ce6-9829-45abe3aff659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "---------------\n",
            "define a rag store A RAG vector store is a database or dataset that contains\n",
            "vectorized data points.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_formatted_response(augmented_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkyYx_MC7Jwg"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V3srRHW7Jwh",
        "outputId": "4ebad0d9-0b9e-4ef2-883d-e69fc57b2363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:\n",
            "---------------\n",
            "A vector store, often referred to as a vector database or vector dataset, is a\n",
            "specialized type of database designed to store and manage data in the form of\n",
            "vectors. Vectors are mathematical representations of data points, typically in\n",
            "the form of arrays or lists of numbers. These vectors can represent a wide range\n",
            "of data types, including text, images, audio, and more, by capturing their\n",
            "essential features in a numerical format.  The primary purpose of a vector store\n",
            "is to facilitate efficient storage, retrieval, and manipulation of vectorized\n",
            "data. This is particularly useful in applications involving machine learning,\n",
            "artificial intelligence, and data analysis, where operations such as similarity\n",
            "search, clustering, and classification are common.  Key characteristics and\n",
            "functionalities of a vector store include:  1. **High-Dimensional Data\n",
            "Handling**: Vector stores are optimized to handle high-dimensional data, which\n",
            "is common in applications like natural language processing (NLP) and computer\n",
            "vision. Each data point is represented as a vector in a multi-dimensional space.\n",
            "2. **Similarity Search**: One of the main features of a vector store is the\n",
            "ability to perform similarity searches. This involves finding vectors that are\n",
            "similar to a given query vector based on a defined similarity metric, such as\n",
            "cosine similarity or Euclidean distance. This is crucial for tasks like\n",
            "recommendation systems and information retrieval.  3. **Scalability**: Vector\n",
            "stores are designed to efficiently manage large volumes of vector data, making\n",
            "them suitable for big data applications. They often include indexing mechanisms\n",
            "to speed up search and retrieval operations.  4. **Integration with Machine\n",
            "Learning**: Vector stores can be integrated with machine learning models to\n",
            "store embeddings generated by these models. Embeddings are vector\n",
            "representations of data that capture semantic meaning, enabling advanced\n",
            "analytics and predictions.  5. **Support for Various Data Types**: While vectors\n",
            "are numerical, they can represent various data types. For example, in NLP, words\n",
            "or sentences can be converted into vectors using techniques like word embeddings\n",
            "or sentence embeddings.  Overall, a vector store is a powerful tool for managing\n",
            "and leveraging vectorized data, enabling advanced data processing and analysis\n",
            "capabilities in various domains.\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function and print the result\n",
        "llm_response = call_llm_with_full_text(augmented_input)\n",
        "print_formatted_response(llm_response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}